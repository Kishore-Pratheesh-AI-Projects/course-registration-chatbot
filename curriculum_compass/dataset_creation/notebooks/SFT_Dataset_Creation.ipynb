{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96559ae7-9afe-4148-87b6-3c5594d3af79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec 28 19:56:37 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              66W / 500W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e40fda7-1b80-4f8b-b2f0-ed027f9e9c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "seed = 3\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d835c555-7cca-4d20-9c87-3bbfc80c3de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRN</th>\n",
       "      <th>Campus Description</th>\n",
       "      <th>Course Title</th>\n",
       "      <th>Subject Course</th>\n",
       "      <th>Faculty Name</th>\n",
       "      <th>Course Description</th>\n",
       "      <th>Term</th>\n",
       "      <th>Begin Time</th>\n",
       "      <th>End Time</th>\n",
       "      <th>Days</th>\n",
       "      <th>Prerequisites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34154</td>\n",
       "      <td>Online</td>\n",
       "      <td>Computer Science and Its Applications</td>\n",
       "      <td>CS1100</td>\n",
       "      <td>Lieberherr, Karl</td>\n",
       "      <td>Introduces students to the field of computer s...</td>\n",
       "      <td>Spring 2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CRN Campus Description                           Course Title  \\\n",
       "0  34154             Online  Computer Science and Its Applications   \n",
       "\n",
       "  Subject Course      Faculty Name  \\\n",
       "0         CS1100  Lieberherr, Karl   \n",
       "\n",
       "                                  Course Description         Term  Begin Time  \\\n",
       "0  Introduces students to the field of computer s...  Spring 2025         NaN   \n",
       "\n",
       "   End Time Days Prerequisites  \n",
       "0       NaN  NaN            []  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_URL = \"../../data_pipeline/notebooks/data/courses.csv\"\n",
    "\n",
    "courses_df = pd.read_csv(DATA_URL)\n",
    "courses_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f9587c-3258-4911-a952-c2115042dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_profs = 50\n",
    "\n",
    "professors = courses_df[\"Faculty Name\"].unique()\n",
    "professors = np.random.choice(professors, size=n_profs, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44af567f-e697-48b8-81ca-8aaf77c88938",
   "metadata": {},
   "outputs": [],
   "source": [
    "semester = \"Spring 2025\"\n",
    "\n",
    "topics = [\n",
    "    \"Machine Learning\",\n",
    "    \"Deep Learning\",\n",
    "    \"Data Science\",\n",
    "    \"Computer Systems\",\n",
    "    \"Programming\",\n",
    "    \"Development\"\n",
    "]\n",
    "\n",
    "course_names = [\n",
    "    'Foundations of Artificial Intelligence',\n",
    "    'Algorithms',\n",
    "    'Programming Design Paradigm',\n",
    "    'Pattern Recognition and Computer Vision',\n",
    "    'Natural Language Processing',\n",
    "    'Deep Learning',\n",
    "    'Machine Learning',\n",
    "    'Information Retrieval',\n",
    "    'Computer Systems',\n",
    "    'Database Management',\n",
    "    'Recitation for CS5010',\n",
    "    'Foundations of Software Engineering',\n",
    "    'Reinforcement Learning',\n",
    "    'Computer/Human Interaction',\n",
    "    'Web Development',\n",
    "    'Compilers',\n",
    "    'Mobile Application Development',\n",
    "    'Discrete Structures',\n",
    "    'Data Mining Techniques',\n",
    "    'Fundamentals of Cloud Computing',\n",
    "    'Object-Oriented Design',\n",
    "    'Computer Systems'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ad526b1-8048-4043-9a84-3142d0c1c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_query_list = [\n",
    "    \"What courses are being offered for the {semester} semester?\",\n",
    "    \"Which professor is taking the course {course_name}?\",\n",
    "    \"What are the courses available related to {topic}?\",\n",
    "    \"Can you please suggest online courses to take?\",\n",
    "    \"Can you suggest research or seminar-based courses?\",\n",
    "    \"Can you suggest courses without exams?\",\n",
    "    \"Can you suggest courses which are not too hectic and easy to get good grades?\",\n",
    "    \"What are the class timings for the course {course_name}?\",\n",
    "    \"Are there any prerequisite courses for {course_name}?\",\n",
    "    \"Has the course {course_name} been offered in the past? If so, could you summarize the reviews?\",\n",
    "    \"How's the course {course_name} based on the reviews?\",\n",
    "    \"How difficult is the course {course_name}?\",\n",
    "    \"What courses are being offered by Professor {professor_name}?\",\n",
    "    \"How is Professor {professor_name} in terms of grading?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5afa887-4d18-4606-80f3-12dfbf88e9a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m sft_dataset_creator_system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mYou are Qwen, a SFT Dataset creation tool. Given a question and context related to the question, you generate responses based on them.\u001b[39m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;132;01m{generated_response}\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcontext\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m variant_system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124mYou are Qwen, an LLM to help generate multiple variants of a question.\u001b[39m\n\u001b[1;32m     17\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m}\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'context' is not defined"
     ]
    }
   ],
   "source": [
    "sft_dataset_creator_system_prompt = \"\"\"\n",
    "You are Qwen, a SFT Dataset creation tool. Given a question and context related to the question, you generate responses based on them.\n",
    "\n",
    "Output Format:\n",
    "Question:\n",
    "{question}\n",
    "Context:\n",
    "{context}\n",
    "Response:\n",
    "{generated_response}\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
    "\n",
    "variant_system_prompt = \"\"\"\n",
    "You are Qwen, an LLM to help generate multiple variants of a question.\n",
    "\n",
    "Given a question and the context, rephrase it in 10 different ways without altering it's meaning.\n",
    "The rephrased question should have the same semantic meaning with different tone, style, etc.\n",
    "\n",
    "Output Format:\n",
    "\n",
    "The output should be in the following format.\n",
    "{\n",
    "    \"question\": [new_question_1, new_question_2, ..., new_question_10]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db01fdd-eb5d-4b03-ba66-d0aa411e394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_dataset = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b60dee-194d-4e63-9568-cdb1704a4696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d48f040-c304-431b-bbcd-a70b739e7d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current allocated GPU memory: 0.00 MB\n",
      "Current cached GPU memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def free_gpu_memory():\n",
    "    # Clear unused objects by forcing garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    # Empty the CUDA cache to release unused GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Print current GPU memory usage for monitoring\n",
    "    print(f\"Current allocated GPU memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"Current cached GPU memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "free_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3ad5813-7772-4ba4-af35-a6582c990f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def generate_response(system_prompt, user_prompt, model_name=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    # Load the model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Prepare the message and text for generation\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize the input and move to appropriate device\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate the output\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "\n",
    "    # Decode and return the response\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2a3a375-6b95-422b-9a02-8c9bf99c9acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1199a804a9ed47df95a5bf59588aa1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm Qwen, your helpful LLM. I'd be delighted to introduce you to large language models.\n",
      "\n",
      "**What is a Large Language Model (LLM)?**\n",
      "\n",
      "A Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language. It's a computer program that's trained on vast amounts of text data, allowing it to learn patterns, relationships, and nuances of language.\n",
      "\n",
      "**How do LLMs work?**\n",
      "\n",
      "LLMs are typically trained using a process called masked language modeling. Here's a simplified overview:\n",
      "\n",
      "1. **Data collection**: A massive dataset of text is gathered, which can include books, articles, conversations, and more.\n",
      "2. **Tokenization**: The text is broken down into individual words or tokens.\n",
      "3. **Model training**: The tokens are fed into the LLM, which learns to predict the missing words or tokens based on the context.\n",
      "4. **Optimization**: The LLM is trained to minimize the error between its predictions and the actual words, using a process called backpropagation.\n",
      "5. **Fine-tuning**: The LLM is fine-tuned for specific tasks, such as language translation, question-answering, or text generation.\n",
      "\n",
      "**What are the benefits of LLMs?**\n",
      "\n",
      "1. **Improved language understanding**: LLMs can comprehend complex language structures, nuances, and context.\n",
      "2. **Natural language generation**: LLMs can generate human-like text, making them useful for applications like chatbots, language translation, and content creation.\n",
      "3. **Scalability**: LLMs can process vast amounts of text data, making them suitable for large-scale applications.\n",
      "\n",
      "**What are the applications of LLMs?**\n",
      "\n",
      "1. **Virtual assistants**: LLMs power virtual assistants like Siri, Alexa, and Google Assistant.\n",
      "2. **Language translation**: LLMs are used for language translation, such as Google Translate.\n",
      "3. **Content generation**: LLMs are used for content creation, like text summarization, article writing, and chatbots.\n",
      "4. **Sentiment analysis**: LLMs can analyze text to determine sentiment, emotions, and opinions.\n",
      "\n",
      "I hope this introduction to large language models has been helpful! Do you have any specific questions or topics you'd like to explore further?\n",
      "Took 12.715358257293701 secs.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "system_prompt = \"You are Qwen, the useful LLM\"\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "\n",
    "start_time = time()\n",
    "response = generate_response(system_prompt, prompt)\n",
    "print(response)\n",
    "end_time = time()\n",
    "\n",
    "print(f\"Took {end_time - start_time} secs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a4136-dadf-47e1-b4f3-2589d84f5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed_query in seed_query_list:\n",
    "    if \"professor_name\" in seed_query:\n",
    "        for professor_name in professors:\n",
    "            new_seed_query = seed_query.format(professor_name=professor_name)\n",
    "            print(new_seed_query)\n",
    "            \n",
    "            # TODO: Make multiple variants of the same question\n",
    "\n",
    "            # TODO: Use the existing RAG Pipeline to fetch the context for the query\n",
    "\n",
    "            # TODO: Prompt an LLM to generate the response\n",
    "\n",
    "            # TODO: Save the question, context and response\n",
    "\n",
    "    elif \"semester\" in seed_query:\n",
    "        new_seed_query = seed_query.format(semester=semester)\n",
    "        print(new_seed_query)\n",
    "\n",
    "        # TODO: Make multiple variants of the same question\n",
    "\n",
    "        # TODO: Use the existing RAG Pipeline to fetch the context for the query\n",
    "\n",
    "        # TODO: Prompt an LLM to generate the response\n",
    "\n",
    "        # TODO: Save the question, context and response\n",
    "\n",
    "    elif \"course_name\" in seed_query:\n",
    "        for course_name in course_names:\n",
    "            new_seed_query = seed_query.format(course_name=course_name)\n",
    "            print(new_seed_query)\n",
    "\n",
    "            # TODO: Make multiple variants of the same question\n",
    "\n",
    "            # TODO: Use the existing RAG Pipeline to fetch the context for the query\n",
    "\n",
    "            # TODO: Prompt an LLM to generate the response\n",
    "\n",
    "            # TODO: Save the question, context and response\n",
    "\n",
    "    elif \"topic\" in seed_query:\n",
    "        for topic in topics:\n",
    "            new_seed_query.format(topic=topic)\n",
    "            print(new_seed_query)\n",
    "\n",
    "            # TODO: Make multiple variants of the same question\n",
    "\n",
    "            # TODO: Use the existing RAG Pipeline to fetch the context for the query\n",
    "\n",
    "            # TODO: Prompt an LLM to generate the response\n",
    "\n",
    "            # TODO: Save the question, context and response\n",
    "\n",
    "    else:\n",
    "        # TODO: Make multiple variants of the same question\n",
    "\n",
    "        # TODO: Use the existing RAG Pipeline to fetch the context for the query\n",
    "\n",
    "        # TODO: Prompt an LLM to generate the response\n",
    "\n",
    "        # TODO: Save the question, context and response\n",
    "        print(seed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5cf8a-2df2-47b1-bf23-2b4caad22946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
